{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의도분류 모델 만들기 (학습)\n",
    "train data는 total_train_data.csv입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 \n",
    "참고 : tf.data를 활용하는 법    \n",
    "https://blog.naver.com/gyungsumin/221737330099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "train_file = \"total_train_data.csv\"\n",
    "data = pd.read_csv(train_file, delimiter=',') # delimiter는 구분자 (seperator) csv파일에서는 굳이 구분자 ,를 지정해주지 않아도 잘 불러오긴 함!\n",
    "queries = data['query'].tolist()\n",
    "intents = data['intent'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['헬로우',\n",
       " '헬로',\n",
       " '안부 인사드립니다.',\n",
       " '먼저 인사하려고 했는데 짝남이 먼저 인사해줬어. 더 떨렸겠어요.',\n",
       " '먼저 인사할까 했는데 짝녀가 먼저 인사해줬어. 기분 좋았겠네요.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>헬로우</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>헬로</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>안부 인사드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>먼저 인사하려고 했는데 짝남이 먼저 인사해줬어. 더 떨렸겠어요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>먼저 인사할까 했는데 짝녀가 먼저 인사해줬어. 기분 좋았겠네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 query  intent\n",
       "0                                  헬로우       0\n",
       "1                                   헬로       0\n",
       "2                           안부 인사드립니다.       0\n",
       "3  먼저 인사하려고 했는데 짝남이 먼저 인사해줬어. 더 떨렸겠어요.       0\n",
       "4  먼저 인사할까 했는데 짝녀가 먼저 인사해줬어. 기분 좋았겠네요.       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/zhenxi/Desktop/for_git/AIB_PJ1/AIB_PJ1_QnA-chatbot/chatbot')\n",
    "from utils.Preprocess import Preprocess\n",
    "p = Preprocess(word2index_dic='../../train_tools/dict/chatbot_dict.bin',\n",
    "               userdic='../../utils/user_dic.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 시퀀스 생성\n",
    "# queries에서 한문장문장씩 꺼내서 인덱스로 반환한다.\n",
    "\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    pos = p.pos(sentence)\n",
    "    keywords = p.get_keywords(pos, without_tag=True)\n",
    "    seq = p.get_wordidx_sequence(keywords)\n",
    "    sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1],\n",
       " [6708],\n",
       " [2967, 1051, 66],\n",
       " [684, 1051, 2, 567, 896, 684, 1051, 8, 149, 1135],\n",
       " [684, 1051, 2, 567, 666, 684, 1051, 8, 732, 162]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 작업\n",
    "# 단어 인덱스 시퀀스 벡터 생성\n",
    "# 단어 시퀀스 벡터 크기 : config를 참고하면 MAX_SEQ_LEN은 15이다. \n",
    "from config.GlobalParams import MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [6708,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [2967, 1051,   66,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [ 684, 1051,    2,  567,  896,  684, 1051,    8,  149, 1135,    0,\n",
       "           0,    0,    0,    0],\n",
       "       [ 684, 1051,    2,  567,  666,  684, 1051,    8,  732,  162,    0,\n",
       "           0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seqs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-20 13:33:10.103714: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# 학습용, 검증용, 테스트용 데이터셋 생성\n",
    "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
    "\n",
    "# tf.data.Dataset은 대량의 데이터를 표현할 수 있는 API\n",
    "# from_tensor_slices에 두개의 리스트를 넣어줌 -> 같은 인덱스의 값끼리 튜플로 엮임 -> data split을 한번에 할 수 있다. \n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, intents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(15,), dtype=int32, numpy=\n",
       "  array([ 162,  615,   19,  779, 1215,   19,  366,  539,   26,   25,  845,\n",
       "          295,   19,   65,   98], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(), dtype=int32, numpy=4>),\n",
       " (<tf.Tensor: shape=(15,), dtype=int32, numpy=\n",
       "  array([ 514,  649,   94, 7525,  649,    2,  293,  572,    0,    0,    0,\n",
       "            0,    0,    0,    0], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(), dtype=int32, numpy=4>),\n",
       " (<tf.Tensor: shape=(15,), dtype=int32, numpy=\n",
       "  array([11236,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(), dtype=int32, numpy=4>),\n",
       " (<tf.Tensor: shape=(15,), dtype=int32, numpy=\n",
       "  array([11237,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(), dtype=int32, numpy=4>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from_tensor_slices 결과 확인\n",
    "list(ds)[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.shuffle(len(queries)) # 데이터를 랜덤으로 섞음\n",
    "\n",
    "# dataset = dataset.shuffle(len(sequences))\n",
    " # shuffle 함수의 인자는 데이터의 전체 길이를 고정으로 넣어 줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73960, 21131, 10565)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size, val_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.word_index) + 1  # 전체 단어 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17751, 15)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE, MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 정의\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "# keras emddeing 인자 (총 단어 수, 임베딩해서 나올 사이즈, 패딩처리된 시퀀스 벡터의 크기)\n",
    "# 임베딩은 왜 해주는 걸까?\n",
    "\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer) # 과적합 방지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv1D를 이용하는 이유는 \n",
    "글자를 1차원 데이터로 처리하기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb) # input이 dropout_emb\n",
    "pool1 = GlobalMaxPool1D()(conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = Conv1D(\n",
    "    filters=128, # 필터 개수\n",
    "    kernel_size=4, # 필터 사이즈 \n",
    "    padding='valid', # valid = 패딩 없음\n",
    "    activation=tf.nn.relu)(dropout_emb) # 활성화 함수는 ReLU \n",
    "pool2 = GlobalMaxPool1D()(conv2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=5,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막 클래스 분류 작업을 위해 3개의 feature map을 묶어준다.(합친다!)\n",
    "concat = concatenate([pool1, pool2, pool3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 384) dtype=float32 (created by layer 'concatenate_1')>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특징 추출이 끝났으니, 클래스 분류를 해야한다.   \n",
    "Dense layer(hidden)를 추가하고, drop out도 해주며 마지막에 5개의 의도 중 하나로 분류 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(5, name='logits')(dropout_hidden)\n",
    "predictions = Dense(5, activation=tf.nn.softmax)(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3698/3698 [==============================] - 71s 19ms/step - loss: 0.0461 - accuracy: 0.9860 - val_loss: 0.0128 - val_accuracy: 0.9930\n",
      "Epoch 2/5\n",
      "3698/3698 [==============================] - 70s 19ms/step - loss: 0.0146 - accuracy: 0.9944 - val_loss: 0.0134 - val_accuracy: 0.9951\n",
      "Epoch 3/5\n",
      "3698/3698 [==============================] - 76s 20ms/step - loss: 0.0124 - accuracy: 0.9956 - val_loss: 0.0076 - val_accuracy: 0.9970\n",
      "Epoch 4/5\n",
      "3698/3698 [==============================] - 72s 20ms/step - loss: 0.0101 - accuracy: 0.9965 - val_loss: 0.0065 - val_accuracy: 0.9970\n",
      "Epoch 5/5\n",
      "3698/3698 [==============================] - 71s 19ms/step - loss: 0.0082 - accuracy: 0.9966 - val_loss: 0.0069 - val_accuracy: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8892883e20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529/529 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9973\n",
      "Accuracy: 99.734974\n",
      "loss: 0.005362\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가(테스트 데이터셋 이용)\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy * 100))\n",
    "print('loss: %f' % (loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save('intent_model.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5dea58ac3b4bbebbd91515097ac691a075b9a2e0b820701f451032ea2ddaae22"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('PJ1-2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
